# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DMAkykc-JopvRXZQSJvHIYos1XNLv-Wb

## I. Write a basic implementation of Lloyd’s algorithm for a set of data in Rd (i.e., to find a Voronoi partition and a set of K centroids). Your algorithm should attempt to solve the classic K-means problem, for any user-selected positive integer value K.

* Assume the input data is given to you in a matrix X ∈ RN×d, where each row in X corresponds to an observation of a d-dimensional point. That is, your inputs will be a user-provided matrix **X** and the number of clusters K.
* Your outputs should be (i) a matrix Y ∈ RK×d, where row j contains the centroid of the jth partition; (ii) a cluster index vector C ∈ {1, 2, . . . K}
N , where C(i) = j indicates that the ith row of X (or the ith observation xi) belongs to cluster j; and (iii) the final objective function value, i.e., the best distortion, or averaged distance value, D obtained.
* Convergence may be based on a norm-based comparison of the iterates of Y , i.e., ∥Yp+1 −Yp∥ < tol, OR on a norm-based comparison of the distortion achieved ∥Dp+1 − Dp∥ < tol. Choose tolto be (1) 1 × 10−5, and (2) a different value of your choice, with your reasoning provided.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

shapeddata_df = pd.read_csv("/content/drive/MyDrive/IE529_comp2/ShapedData.csv", header=None, names=['x','y'])
clustering_df = pd.read_csv("/content/drive/MyDrive/IE529_comp2/clustering.csv", header=None, names=['x','y'])

"""## A Simple K-Means Lloyd’s algorithm to find K-Clusters based on user given value for K."""

# Simple Lloyd's algorithm
def lloydalgorithm(data_points, k, tolerance, maximum_iteration=10000):
  random_index = np.random.choice(data_points.shape[0], k, replace=False)
  k_centroids = data_points[random_index]
  for i in range(maximum_iteration):
    old_centroids = k_centroids
    diff = data_points - k_centroids.reshape(k_centroids.shape[0], 1, k_centroids.shape[1])
    dist = np.sqrt((diff**2).sum(axis=2))
    closest_pt = np.argmin(dist, axis=0)
    k_centroids = np.array([data_points[closest_pt==i].mean(axis=0) for i in range(k)])
    if np.all(np.abs(k_centroids - old_centroids)<tolerance):
        break
  best_distortion = 0
  for i, point_val in enumerate(k_centroids):
        cluster_points = data_points[closest_pt==i]
        best_distortion = best_distortion+np.sum((cluster_points-point_val)**2)
  return closest_pt, k_centroids,best_distortion

"""# Answer for Clustering_data for K=4 and for Tolerance Value : 1e-5
* (i) a matrix of centroids
* (ii) a cluster index vector
* (iii) the final objective function value, i.e., the best distortion
"""

labels, centroids, distortion = lloydalgorithm(np.array(clustering_df),4,1e-5)
print("(i) a matrix of centroids : \n",centroids)
print("\n(ii) a cluster index vector :\n", labels)
print("\nBest Distortion value: ",distortion/len(np.array(clustering_df)))

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=labels, cmap='viridis', marker='o')
plt.scatter(centroids[:, 0], centroids[:, 1], s=50, c='black', marker='+', label='Centroids')
plt.title('Lloyd\'s Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.legend()
plt.grid(True)
plt.show()

"""# Answer for shaped_data for K=4 and for Tolerance Value : 1e-5
* (i) a matrix of centroids
* (ii) a cluster index vector
* (iii) the final objective function value, i.e., the best distortion
"""

labels1, centroids1, distortion1 = lloydalgorithm(np.array(shapeddata_df),4,1e-5)
print("(i) a matrix of centroids : \n",centroids1)
print("\n(ii) a cluster index vector :\n", labels1)
print("\nBest Distortion value: ",distortion1/len(np.array(shapeddata_df)))

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=labels1, cmap='viridis', marker='o')
plt.scatter(centroids1[:, 0], centroids1[:, 1], s=50, c='black', marker='+', label='Centroids')
plt.title('Lloyd\'s Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.legend()
plt.grid(True)
plt.show()

"""# For Tolerance Value : $1e^-1$

Justification: I have choosen the tolerance value as 1 because, I we can see signication changes in the data points assigning to randomly choosen first centroid. But when you decrease it even more than $1e^-5$ is usefull for higher dimensional data to converge more correclty, but in our dataset as the data is 2 dimension we cannot see significant change in points assigning to the cluster. But when we increase the tolerance value like close to one $1e^-1$ we get different points assigned to cluster centers.

Answer for Clustering_data for K=4
"""

labels_tor, centroids_tor, distortion_tor = lloydalgorithm(np.array(clustering_df),4,1e-1)
print("(i) a matrix of centroids : \n",centroids_tor)
print("\n(ii) a cluster index vector :\n", labels_tor)
print("\nBest Distortion value: ",distortion_tor/len(np.array(clustering_df)))

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=labels_tor, cmap='viridis', marker='o')
plt.scatter(centroids_tor[:, 0], centroids_tor[:, 1], s=50, c='black', marker='+', label='Centroids')
plt.title('Lloyd\'s Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.legend()
plt.grid(True)
plt.show()

"""Answer for shaped_data for K=4"""

labels1_tor, centroids1_tor, distortion1_tor = lloydalgorithm(np.array(shapeddata_df),4,1)
print("(i) a matrix of centroids : \n",centroids1_tor)
print("\n(ii) a cluster index vector :\n", labels1_tor)
print("\nBest Distortion value: ",distortion1_tor/len(np.array(shapeddata_df)))

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=labels1_tor, cmap='viridis', marker='o')
plt.scatter(centroids1_tor[:, 0], centroids1_tor[:, 1], s=50, c='black', marker='+', label='Centroids')
plt.title('Lloyd\'s Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.legend()
plt.show()

"""## Greedy K-Centers Algorithm Implementation

Write a basic implementation of the "Greedy K-Centers" algorithm, as described in the reading by S. Har-Peled and discussed in class. Your algorithm should aim to solve the classic K-centers problem for any user-selected positive integer value K. The underlying distance function used in your algorithm should be the Euclidean distance, and your objective should be to minimize the maximum distance between any observation $x_i \in X $ and its closest center $(c_j) \in Q $. That is, to find Q giving:

$min_{Q \subseteq X, |Q| = K} \left( \max_{x_i \in X} \left(\min_{c_j \in Q} \|x_i - c_j\|_2 \right) \right)$

### Assumptions
- The input data is given as a matrix $ X \in \mathbb{R}^{N \times d} $.
- A positive integer \( K \).

### Output
- Your output should be a matrix $ Q \in \mathbb{R}^{K \times d} $ containing the final \( K \) d-dimensional centers.
- The objective function value, i.e., the final $ \max_{x_i \in X} (\min_{c_j \in Q} \|x_i - c_j\|_2) $ obtained.

K-Greedy Algorithm is as follows:
"""

def greedykalgo1(data_points, k):
  first_index = np.random.choice(data_points.shape[0], 1, replace=False)
  #print(first_index)
  k_centers = data_points[first_index]
  current_cent = data_points[first_index]
  #print(current_cent)
  #print(data_points.shape)
  #data_points = np.delete(data_points, first_index,axis=0)
  #print(data_points.shape)
  #print(k_centers)
  for i in range(1,k):
    diff = data_points - current_cent
    dist = np.sqrt((diff**2).sum(axis=1))
    max_dist = np.max(dist)
    center_index = np.argmax(dist)
    #print(center_index," \n")
    k_centers=np.append(k_centers, [data_points[center_index]], axis=0)
    current_cent = data_points[[center_index]]
    # data_points = np.delete(data_points, center_index,axis=0)
  return k_centers, max_dist #,data_points

"""For Clustered_data with k=3:"""

cent_k, max_disttt = greedykalgo1(np.array(clustering_df),3)
print("K_centers :\n",cent_k)
print("\nBest Maximum Distance:",max_disttt)

def labelscal(data_points, centers):
    diff1 = data_points - centers.reshape(centers.shape[0], 1, centers.shape[1])
    dist1 = np.sqrt((diff1**2).sum(axis=2))
    closest_pt2 = np.argmin(dist1, axis=0)
    return closest_pt2

close_points_gk3 = labelscal(np.array(clustering_df),cent_k)
print("Assigned Cluster Labels:\n", close_points_gk3)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=close_points_gk3, cmap='viridis', marker='o')
plt.scatter(cent_k[:, 0], cent_k[:, 1], s=70, c='black', marker='+', label='Centroids')
plt.title('Greedy K Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.legend()
plt.grid(True)
plt.show()

"""For Shaped_data with k=3:"""

cent_k1, max_disttt1 = greedykalgo1(np.array(shapeddata_df),3)
print("K_centers :\n",cent_k1)
print("\nBest Maximum Distance:",max_disttt1)

close_points_gk2 = labelscal(np.array(shapeddata_df),cent_k1)
print("Assigned Cluster Labels:\n", close_points_gk2)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=close_points_gk2, cmap='viridis', marker='o')
plt.scatter(cent_k1[:, 0], cent_k1[:, 1], s=70, c='black', marker='+', label='Centroids')
plt.title('Greedy K Algorithm with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.legend()
plt.grid(True)
plt.show()

"""# II.2 - Extra Credit:
Write a basic implementation of the single-swap heuristic for which you try to improve the solution to the K-centers problem in II.1 by a implementing a series of ”swaps”. If Q is your current set of centers, and you make a single swap, giving Qnew = Q − {cj} ∪ {o}, then you should replace Q with Qnew whenever the new objective value, that is the computed value for (1), is reduced by a factor of (1 − τ ). When there is no swap that improves the solution by this factor, the local search stops. Let τ = 0.05.

"""

def singleswapheuristic(data_points, k_centers,max_dist, tau=0.05):
    condition = True
    while condition:
        condition = False
        for i, center in enumerate(k_centers):
            for point in data_points:
                new_centers = np.copy(k_centers)
                new_centers[i] = point
                new_dist = np.max(np.min(np.linalg.norm(data_points[:, np.newaxis] - new_centers, axis=2), axis=1))
                if new_dist < max_dist * (1 - tau):
                    print("Replacing center",i,"with point:",point)
                    print("New max distance:",new_dist)
                    k_centers = new_centers
                    max_dist = new_dist
                    condition = True
                    break
            if condition:
                break
        if not condition:
            print("No improvement stopped")
    return k_centers, max_dist

"""# Now applying single swap heuristic to the above found centers using K-Greedy method for both the datasets to minimize the best maximum distance.

For Clustered_data with k=5:
"""

cent_k_sw, max_disttt_sw = singleswapheuristic(np.array(clustering_df),cent_k,max_disttt)
print("New K_centers :\n",cent_k_sw)
print("\nOld Best Maximum Distance:",max_disttt)
print("\nNew Best Maximum Distance:",max_disttt_sw)

"""For Shaped_data with k=5:"""

cent_k1_sw, max_disttt1_sw = singleswapheuristic(np.array(shapeddata_df),cent_k1,max_disttt1)
print("New K_centers :\n",cent_k1_sw)
print("\nOld Best Maximum Dsitance:",max_disttt1)
print("\nNew Best Maximum Distance:",max_disttt1_sw)

"""### III. Spectral Clustering Algorithm Implementation

Write an implementation of the Spectral Clustering algorithm, using either basic unnormalized clustering or normalized clustering (refer to the reading by Luxborg for details). Assume you are given a matrix of data $( X \in \mathbb{R}^{N \times d})$, and you would like to identify some user-selected number of clusters, \( K \). Your outputs should be:

- A **weighted adjacency matrix, \( W \)**, using the Gaussian similarity function based on the Euclidean distance (with parameter value $( sigma )$ of your choice but clearly stated) and a \( k \)-nearest neighborhood structure (where \( k \) is also your choice and clearly stated);
- A **matrix \( U \)** containing the first \( K \) eigenvectors of the Laplacian \( L \) (or generalized eigenvectors for the normalized case);
- A **cluster index vector $ C \in \{1, 2, \ldots, K\}^N$**, where $ C(i) = j$ indicates that the $i-th$ row of U belongs to cluster j.

This is to find the similarity matrix for the spectral clustering using K-nearest neighborhood structure. The similarity function I am using here is gaussian similarity function,as to find the similarity between two points we can use any of the function like euclidean distance, manhatten etc...
"""

#similarty mat knn
def similarity_mat_knn(data_points, k,sigma=0.5):
  neighbour = NearestNeighbors(n_neighbors=k+1)
  neighbour.fit(data_points)
  distances, indices = neighbour.kneighbors(data_points)
  #print(indices)
  s_mat = np.zeros((len(data_points),len(data_points)))
  for i in range(len(data_points)):
    for j in range(len(data_points)):
      if j in indices[i]:
        temp_dist = np.sum((data_points[i]-data_points[j])**2)
        s_mat[i,j]=np.exp(-temp_dist/(2*sigma**2))
  return s_mat

"""This is to find the similarity matrix for the spectral clustering using the fully connected graph. The similarity function I am using here is gaussian similarity function: $s(x_i, x_j) =exp(−||x_i − xj||^2 /(2σ^2))$. <br><br>
*Choice of sigma value:*<br>
Also the σ value i have choosen is 0.5 for the Gaussian similarity function helps the algorithm pay close attention to smaller distances between data points. This  is great for datasets with uniformly scaled features, as it quickly distinguishes between points based on subtle differences. It's especially useful when you're dealing with data where small variations are significant and you want the algorithm to capture these.
"""

#similarity matrix
def similarity_mat(data_points,sigma=0.5):
  s_mat = np.zeros((len(data_points),len(data_points)))
  for i in range(len(data_points)):
    for j in range(len(data_points)):
      temp_dist = np.sum((data_points[i]-data_points[j])**2)
      s_mat[i,j]=np.exp(-temp_dist/(2*sigma**2))
  return s_mat

#DMatrix
def d_mat(matrix_d):
  d_mat = np.zeros((matrix_d.shape[0],matrix_d.shape[1]))
  for i in range(matrix_d.shape[0]):
    d_mat[i,i]=np.sum(matrix_d[i])
  return d_mat

"""Algorithm for Spectral Clustering:"""

#spectral clustering algorithm
def spectral_cluster(data_points,k,algo,KnnK):
  if algo==1:
    simi_mat = similarity_mat(data_points)
  elif algo==0:
    simi_mat = similarity_mat_knn(data_points,KnnK)
  #print(simi_mat,'\n')
  D_matrix = d_mat(simi_mat)
  #print(D_matrix,'\n')
  Lap_matrix = D_matrix-simi_mat
  #print(Lap_matrix,'\n')
  eigen_val, eigen_vec = np.linalg.eig(Lap_matrix)
  sorted_eigenvalues = eigen_val[np.argsort(eigen_val)]
  sorted_eigenvectors = eigen_vec[:, np.argsort(eigen_val)]
  eigenvec_topk = sorted_eigenvectors[:, :k]
  #print(eigenvec_topk,'--> eigenvec')
  c_pt, k_cen,best_dis = lloydalgorithm(eigenvec_topk,k,1e-5)
  return c_pt, k_cen,best_dis,simi_mat, eigenvec_topk

"""# For Cluster_data with K=4 and using Gaussian similarity function based on the Euclidean distance


"""

lab, k_cen, best_dis, adj_mat, eigen_vec= spectral_cluster(np.array(clustering_df),4,1)

print("Weighted adjacency matrix, W:\n",adj_mat)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec)
print("\nCluster index vector C:\n",lab)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=lab, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using Gaussian similarity function based on the Euclidean distance with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""For Cluster_data with K=5 and using K-Nearest Neighborhood Structure with 5-neighbors"""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(clustering_df),4,0,5)

print("Weighted adjacency matrix, W:\n",adj_mat_knn)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec_knn)
print("\nCluster index vector C:\n",lab_knn)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""Now, increasing the Neighbors to a high value like 50 to see how it affects the cluster assignment."""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(clustering_df),4,0,50)

print("Weighted adjacency matrix, W:\n",adj_mat_knn)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec_knn)
print("\nCluster index vector C:\n",lab_knn)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""On increasing the cluster to 10"""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(clustering_df),10,0,50)
plt.figure(figsize=(10, 6))
plt.scatter(np.array(clustering_df)[:, 0], np.array(clustering_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""Inference: <br>
On Incresing the K Neighbors we can see that the points are properly clustered for small cluster value. But when we increase the cluster to big number, the algorithm clusters the points that are together and the points that are away from other points are termed as cluster like the outliers.

# For Shaped_data with K=4 and using Gaussian similarity function based on the Euclidean distance
"""

lab, k_cen, best_dis, adj_mat, eigen_vec= spectral_cluster(np.array(shapeddata_df),4,1,5)

print("Weighted adjacency matrix, W:\n",adj_mat)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec)
print("\nCluster index vector C:\n",lab)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=lab, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using Gaussian similarity function based on the Euclidean distance with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""#For shaped_data with K=5 and using K-Nearest Neighborhood Structure with 5-neighbors"""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(shapeddata_df),4,0,5)

print("Weighted adjacency matrix, W:\n",adj_mat_knn)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec_knn)
print("\nCluster index vector C:\n",lab_knn)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""Now, increasing the Neighbors to a high value like 50 to see how it affects the cluster assignment."""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(shapeddata_df),4,0,50)

print("Weighted adjacency matrix, W:\n",adj_mat_knn)
print("\nFirst K eigenvectors of the Laplacian L:\n",eigen_vec_knn)
print("\nCluster index vector C:\n",lab_knn)

plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""# As you can see above, the spectral algorithm is clustering the shaped data correctly for 50 nearest neighbour.

On Increasing the Number of clusters to 10
"""

lab_knn, k_cen_knn, best_dis_knn, adj_mat_knn, eigen_vec_knn= spectral_cluster(np.array(shapeddata_df),10,0,50)
plt.figure(figsize=(10, 6))
plt.scatter(np.array(shapeddata_df)[:, 0], np.array(shapeddata_df)[:, 1], c=lab_knn, cmap='viridis', marker='o')
plt.title('Spectral Algorithm using K-nearest neighborhood structure with k Clusters')
plt.xlabel('Feature 1(x)')
plt.ylabel('Feature 2(y)')
plt.grid(True)
plt.show()

"""Inference: <br>
On Incresing the K Neighbors we can see that the points are less properly clustered for small cluster value. But when we increase the cluster to big number, the algorithm clusters the points into perfect clusters and can be seen in the shaped data. From visual we can see there are 4 clusters, as I have given 10 clusters in the algorithm; it groups the 1 of the 4 clusters into segments totally adding to 10.
"""
